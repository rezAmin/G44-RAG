{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG Chatbot — University Regulations (Sharif University)\n",
        "\n",
        "This notebook runs the full RAG pipeline:\n",
        "1. Install dependencies\n",
        "2. Upload/load knowledge base chunks\n",
        "3. Build embeddings + FAISS index\n",
        "4. Retrieval\n",
        "5. LLM generation with Qwen2.5-7B-Instruct (4-bit)\n",
        "6. Interactive chat + Evaluation\n",
        "\n",
        "**Runtime:** Use GPU (T4 or better) via Runtime > Change runtime type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install -q torch transformers sentence-transformers faiss-cpu bitsandbytes accelerate gradio tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Upload Knowledge Base\n",
        "\n",
        "Upload `sharif_rules_chunks.json` from your `data/` folder, or clone the repo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# Option A: Clone repo (uncomment if using git)\n",
        "# !git clone https://github.com/YOUR_USERNAME/G44-RAG.git\n",
        "# os.chdir('G44-RAG')\n",
        "\n",
        "# Option B: Upload file manually\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # upload sharif_rules_chunks.json\n",
        "\n",
        "CHUNKS_PATH = 'sharif_rules_chunks.json'\n",
        "\n",
        "with open(CHUNKS_PATH, 'r', encoding='utf-8') as f:\n",
        "    chunks = json.load(f)\n",
        "\n",
        "print(f'Loaded {len(chunks)} chunks')\n",
        "print(f'Sample chunk keys: {list(chunks[0].keys())}')\n",
        "print(f'Sample content: {chunks[1][\"content\"][:200]}...')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Build Embeddings + FAISS Index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "\n",
        "EMBEDDING_MODEL_NAME = 'intfloat/multilingual-e5-base'\n",
        "\n",
        "print(f'Loading embedding model: {EMBEDDING_MODEL_NAME}')\n",
        "embed_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
        "\n",
        "def prepare_texts(chunks):\n",
        "    texts = []\n",
        "    for chunk in chunks:\n",
        "        header = f\"{chunk['rule_title']} — {chunk['section_title']}\"\n",
        "        text = f\"passage: {header}\\n{chunk['content']}\"\n",
        "        texts.append(text)\n",
        "    return texts\n",
        "\n",
        "texts = prepare_texts(chunks)\n",
        "print(f'Prepared {len(texts)} texts for embedding')\n",
        "\n",
        "print('Computing embeddings...')\n",
        "embeddings = embed_model.encode(texts, batch_size=32, show_progress_bar=True)\n",
        "embeddings = np.array(embeddings, dtype='float32')\n",
        "print(f'Embeddings shape: {embeddings.shape}')\n",
        "\n",
        "faiss.normalize_L2(embeddings)\n",
        "dim = embeddings.shape[1]\n",
        "index = faiss.IndexFlatIP(dim)\n",
        "index.add(embeddings)\n",
        "print(f'FAISS index built with {index.ntotal} vectors (dim={dim})')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Retrieval Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def retrieve(query, top_k=5):\n",
        "    query_text = f'query: {query}'\n",
        "    query_embedding = embed_model.encode([query_text], normalize_embeddings=True).astype('float32')\n",
        "    scores, indices = index.search(query_embedding, top_k)\n",
        "\n",
        "    results = []\n",
        "    for rank, (idx, score) in enumerate(zip(indices[0], scores[0])):\n",
        "        if idx < 0:\n",
        "            continue\n",
        "        chunk = chunks[idx].copy()\n",
        "        chunk['score'] = float(score)\n",
        "        chunk['rank'] = rank + 1\n",
        "        results.append(chunk)\n",
        "    return results\n",
        "\n",
        "\n",
        "def format_context(results):\n",
        "    parts = []\n",
        "    for r in results:\n",
        "        header = f\"[{r['rule_title']} | {r['section_title']}]\"\n",
        "        parts.append(f\"{header}\\n{r['content']}\")\n",
        "    return '\\n\\n---\\n\\n'.join(parts)\n",
        "\n",
        "\n",
        "# Test retrieval\n",
        "test_query = 'شرایط مشروطی دانشجو چیست؟'\n",
        "results = retrieve(test_query, top_k=3)\n",
        "print(f'Query: {test_query}\\n')\n",
        "for r in results:\n",
        "    print(f\"[Rank {r['rank']}] Score: {r['score']:.4f}\")\n",
        "    print(f\"  Rule: {r['rule_title']}\")\n",
        "    print(f\"  Section: {r['section_title']}\")\n",
        "    print(f\"  Content: {r['content'][:150]}...\")\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Load LLM (Qwen2.5-7B-Instruct, 4-bit)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "MODEL_NAME = 'Qwen/Qwen2.5-7B-Instruct'\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "print(f'Loading model: {MODEL_NAME} (4-bit quantized)...')\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "model.eval()\n",
        "print('Model loaded successfully!')\n",
        "print(f'Device: {model.device}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Generation (Prompt Engineering)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "SYSTEM_PROMPT = \"\"\"تو یک دستیار هوشمند دانشگاهی هستی که فقط بر اساس آیین‌نامه‌ها و مقررات رسمی دانشگاه صنعتی شریف پاسخ می‌دهی.\n",
        "\n",
        "قوانین پاسخ‌دهی:\n",
        "۱. فقط بر اساس متن مقررات ارائه‌شده پاسخ بده. هرگز اطلاعاتی خارج از این متون اضافه نکن.\n",
        "۲. در پاسخ، حتماً نام آیین‌نامه و در صورت امکان شماره ماده، بند یا تبصره را ذکر کن.\n",
        "۳. پاسخ را کوتاه، دقیق و مستقیم بنویس.\n",
        "۴. اگر پاسخ سوال در متون ارائه‌شده وجود ندارد، بگو: «اطلاعاتی در مورد این سوال در آیین‌نامه‌های موجود یافت نشد. لطفاً از اداره آموزش استعلام بگیرید.»\n",
        "۵. هرگز قانون یا تفسیری از خودت اختراع نکن.\n",
        "۶. به زبان فارسی و رسمی پاسخ بده.\"\"\"\n",
        "\n",
        "\n",
        "def generate_answer(query, context, max_new_tokens=512, temperature=0.3):\n",
        "    user_message = f\"\"\"بر اساس متون آیین‌نامه‌ای زیر، به سوال دانشجو پاسخ بده.\n",
        "\n",
        "--- متون مرتبط ---\n",
        "{context}\n",
        "--- پایان متون ---\n",
        "\n",
        "سوال: {query}\"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {'role': 'system', 'content': SYSTEM_PROMPT},\n",
        "        {'role': 'user', 'content': user_message},\n",
        "    ]\n",
        "\n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(text, return_tensors='pt').to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            repetition_penalty=1.1,\n",
        "        )\n",
        "\n",
        "    new_tokens = output_ids[0][inputs['input_ids'].shape[1]:]\n",
        "    return tokenizer.decode(new_tokens, skip_special_tokens=True).strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Full RAG Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def rag_answer(query, top_k=5):\n",
        "    retrieved = retrieve(query, top_k=top_k)\n",
        "    context = format_context(retrieved)\n",
        "    answer = generate_answer(query, context)\n",
        "\n",
        "    return {\n",
        "        'query': query,\n",
        "        'answer': answer,\n",
        "        'sources': [\n",
        "            {\n",
        "                'rule_title': r['rule_title'],\n",
        "                'section_title': r['section_title'],\n",
        "                'score': r['score'],\n",
        "                'content_preview': r['content'][:200],\n",
        "            }\n",
        "            for r in retrieved\n",
        "        ],\n",
        "    }\n",
        "\n",
        "\n",
        "# Test the full pipeline\n",
        "result = rag_answer('شرایط مشروطی دانشجوی کارشناسی چیست؟')\n",
        "print(f\"Q: {result['query']}\")\n",
        "print(f\"\\nA: {result['answer']}\")\n",
        "print(f\"\\nSources:\")\n",
        "for s in result['sources']:\n",
        "    print(f\"  - {s['rule_title']} | {s['section_title']} (score: {s['score']:.4f})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Interactive Chat (Gradio)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import gradio as gr\n",
        "\n",
        "\n",
        "def chat_fn(query, history):\n",
        "    if not query.strip():\n",
        "        return 'لطفاً سوال خود را وارد کنید.'\n",
        "\n",
        "    result = rag_answer(query, top_k=5)\n",
        "    answer = result['answer']\n",
        "\n",
        "    sources_text = '\\n\\n---\\n**منابع:**\\n'\n",
        "    for s in result['sources']:\n",
        "        sources_text += f\"- {s['rule_title']} — {s['section_title']} (امتیاز: {s['score']:.3f})\\n\"\n",
        "\n",
        "    return answer + sources_text\n",
        "\n",
        "\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chat_fn,\n",
        "    title='چت‌بات راهنمای مقررات آموزشی دانشگاه صنعتی شریف',\n",
        "    description='سوالات خود درباره آیین‌نامه‌ها و مقررات آموزشی را بپرسید.',\n",
        "    examples=[\n",
        "        'شرایط مشروطی دانشجو چیست؟',\n",
        "        'حداکثر سنوات مجاز تحصیل در مقطع کارشناسی چقدر است؟',\n",
        "        'شرایط حذف اضطراری درس چیست؟',\n",
        "        'آیا استفاده از هوش مصنوعی در تکالیف مجاز است؟',\n",
        "        'قوانین کارآموزی چیست؟',\n",
        "    ],\n",
        ")\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Evaluation on Question Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import csv\n",
        "from datetime import datetime\n",
        "\n",
        "EVAL_QUESTIONS = [\n",
        "    'شرایط مشروطی دانشجوی کارشناسی چیست؟',\n",
        "    'حداکثر سنوات مجاز تحصیل در دوره کارشناسی چقدر است؟',\n",
        "    'آیا استفاده از ابزار هوش مصنوعی در تکالیف درسی مجاز است؟',\n",
        "    'شرایط حذف اضطراری درس چیست؟',\n",
        "    'قوانین غیبت در امتحان پایان‌ترم چیست؟',\n",
        "    'شرایط معرفی به استاد چگونه است؟',\n",
        "    'قوانین کارآموزی در دوره کارشناسی چیست؟',\n",
        "    'شرایط مهمانی دانشجو در دانشگاه دیگر چگونه است؟',\n",
        "    'قوانین تغییر رشته در دوره کارشناسی چیست؟',\n",
        "    'شرایط پروژه کارشناسی چگونه است؟',\n",
        "    'آیین‌نامه دوره کوآپ چه مقرراتی دارد؟',\n",
        "    'شرایط انتقال به دانشگاه صنعتی شریف چیست؟',\n",
        "    'مهلت فراغت از تحصیل چقدر است؟',\n",
        "    'حداقل و حداکثر واحد مجاز در هر ترم چقدر است؟',\n",
        "    'شرایط دستیاری آموزشی چیست؟',\n",
        "]\n",
        "\n",
        "print(f'Running evaluation on {len(EVAL_QUESTIONS)} questions...\\n')\n",
        "\n",
        "eval_results = []\n",
        "for i, question in enumerate(EVAL_QUESTIONS, 1):\n",
        "    print(f'[{i}/{len(EVAL_QUESTIONS)}] {question}')\n",
        "    result = rag_answer(question, top_k=5)\n",
        "    eval_results.append(result)\n",
        "    print(f'  → {result[\"answer\"][:120]}...')\n",
        "    print()\n",
        "\n",
        "# Save JSON\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "json_path = f'eval_results_{timestamp}.json'\n",
        "with open(json_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(eval_results, f, ensure_ascii=False, indent=2)\n",
        "print(f'JSON saved: {json_path}')\n",
        "\n",
        "# Save CSV\n",
        "csv_path = f'eval_results_{timestamp}.csv'\n",
        "with open(csv_path, 'w', encoding='utf-8', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(['question', 'answer', 'num_sources', 'source_1_title', 'source_1_section', 'source_1_score'])\n",
        "    for r in eval_results:\n",
        "        top = r['sources'][0] if r['sources'] else {}\n",
        "        writer.writerow([\n",
        "            r['query'], r['answer'], len(r['sources']),\n",
        "            top.get('rule_title', ''), top.get('section_title', ''),\n",
        "            f\"{top.get('score', 0):.4f}\",\n",
        "        ])\n",
        "print(f'CSV saved: {csv_path}')\n",
        "\n",
        "# Download files\n",
        "from google.colab import files\n",
        "files.download(json_path)\n",
        "files.download(csv_path)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}